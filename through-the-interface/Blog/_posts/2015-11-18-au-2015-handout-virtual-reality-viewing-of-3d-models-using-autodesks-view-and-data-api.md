---
layout: "post"
title: "AU 2015 handout: Virtual Reality Viewing of 3D Models Using Autodesk's View and Data API"
date: "2015-11-18 11:54:55"
author: "Kean Walmsley"
categories:
  - "AU"
  - "Autodesk"
  - "HTML"
  - "JavaScript"
  - "PaaS"
  - "Virtual Reality"
  - "Web/Tech"
original_url: "https://www.keanw.com/2015/11/au-2015-handout-virtual-reality-viewing-of-3d-models-using-autodesks-view-and-data-api.html "
typepad_basename: "au-2015-handout-virtual-reality-viewing-of-3d-models-using-autodesks-view-and-data-api"
typepad_status: "Publish"
---

<p><em>I’ve been heads-down wrapping up my AU content, so I thought I’d publish at least some of it here. Here’s the handout for <a href="https://events.au.autodesk.com/connect/sessionDetail.ww?SESSION_ID=9942" target="_blank">my “AU kick-off” class</a> which is at 8am on the first day of the conference – the morning after the ADN party – so despite the fact the class is currently full, we’ll see if that turns out to be the case. ;-)</em></p>  <p><b></b></p>  <p><b>VR comes of age</b></p>  <p>Presenting 2D images that allow the brain to reconstruct 3D has its roots in the early <a href="https://en.wikipedia.org/wiki/Stereoscope" target="_blank">stereoscopes</a> invented in the late 1830s. These were eventually democratized by Sawyers and their iconic View-Master brand, which initially focused on virtual tourism but branched out into entertainment and education. The technology was used by the military during WWII to help train people identifying planes, for instance, as well as being used to help doctors learn anatomy. There are strong parallels between the use of stereoscopes in the 20th century and the use of VR, today.</p>  <p>Static images evolved to be dynamic: the 1950s were the heyday of 3D cinema, and marked the early days of research into VR. Autodesk initially felt the time was right to democratize VR in the late 1980s: we were early innovators in the VR space with the Cyberspace Developers Kit.</p>  <p>For a sense of why this effort ultimately failed, it’s worth considering the Gartner Hype Cycle from 1995 and 2015.</p>  <p>The one from 1995 shows – along with the apparently lamentable state of software that drew splines, back then – that VR was already on its way into the “Trough of Disillusionment”.</p>  <p>   <br /><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee2324970b-pi" target="_blank"><img title="Gartner Hype Cycle 1995" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Gartner Hype Cycle 1995" src="/assets/image_62650.jpg" width="384" height="214" /></a></p>  <p>The equivalent graphic from 2015 shows a different story…</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201bb089243f9970d-pi" target="_blank"><img title="Gartner Hype Cycle 2015" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; border-left: 0px; display: block; padding-right: 0px" border="0" alt="Gartner Hype Cycle 2015" src="/assets/image_437659.jpg" width="501" height="307" /></a></p>  <p>Here we see VR is climbing gradually onto the “Plateau of Productivity” – although, surprisingly for some, it’s still considered to be 5-10 years from being fully viable.</p>  <p>The drivers that have finally allowed VR to head along the path to viability have their roots in the explosion of smartphone technology: investments in high resolution, low latency displays and low power IMUs (Inertial Measurement Units) have been huge enablers for VR, in general.</p>  <p>It’s this technology that is powering all modern VR efforts, even the ones that are currently tethered. In many the ways the most groundbreaking of these – in that it went a huge way towards democratizing VR – was the August 2012 Kickstarter campaign for Oculus Rift:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b8d177e92e970c-pi" target="_blank"><img title="Oculus Rift Kickstarter campaign" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Oculus Rift Kickstarter campaign" src="/assets/image_669973.jpg" width="445" height="333" /></a></p>  <p>The next watershed moment for VR was at Google I/O in July 2014, when Google Cardboard was unveiled. The premise was simple: today’s smartphones have everything you need to do pretty decent VR (and guess what? Moore’s Law says that they’re only going to get better at it). Inserting a smartphone into a simple cardboard case with a couple of plastic lenses brings VR to the masses at the cost of just a few dollars.</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee232c970b-pi" target="_blank"><img title="Google Cardboard" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Google Cardboard" src="/assets/image_148128.jpg" width="500" height="323" /></a></p>  <p>Google Cardboard was laughed off by many, at first. Some changed their minds after trying it for the first time: there is something very compelling about the experience you get with close to no effort. For sure there is a way to go, but as smartphones get designed with VR in mind, things will improve quickly.</p>  <p>The first low-cost, Android-powered, dedicated VR headset has <a href="https://www.kickstarter.com/projects/1714346163/auravisor-virtual-reality-head-mounted-computer">just been fully funded on Kickstarter</a>. Powered, of course, by smartphone tech.</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b8d177e932970c-pi" target="_blank"><img title="AuraVisor" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="AuraVisor" src="/assets/image_905570.jpg" width="444" height="333" /></a></p>  <p><b>Using the View &amp; Data API to implement VR</b></p>  <p>While I was intrigued by the Oculus Rift – and was happy to try both the DK1 and DK2, when they came out – it was only when Google Cardboard launched that I really became passionate about VR. For me the experience is going to have to be untethered – and ultimately web-based – for VR to be really compelling.</p>  <p>My first prototyping efforts for Google Cardboard started before I had access to a Cardboard: I coded up a simple web-page based on the Autodesk View &amp; Data API, and sent the link across to a colleague in San Francisco to test. You can imagine my delight when I finally had the chance to test it – on the drive from SFO to the city – and found the experience to be genuinely three dimensional.</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee2334970b-pi" target="_blank"><img title="Early prototype of the VR app" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Early prototype of the VR app" src="/assets/image_326350.jpg" width="504" height="296" /></a></p>  <p>The initial prototype embedded the View and Data viewing component twice – one for each eye. The trick to creating a 3D effect is to have the camera positioned slightly apart but pointing at the same target. You can either do this with the distance between a human’s eyes – 8 cm or so – or you can do as I did and use a percentage (in my case 4%) of the distance between the camera and the target. This has the advantage of being unit independent and working well for all models.</p>  <p>Here’s the relevant JavaScript function (which you can find as part of <a href="https://github.com/KeanW/vr-party/blob/master/html/js/vr-party-participant.js">this source file</a>) that transfers the camera position from one viewer to the other:</p>  <div style="font-size: 8pt; font-family: courier new; background: white; color: black; line-height: 140%">   <p style="margin: 0px"><span style="color: blue"></span></p>    <p style="margin: 0px"><span style="color: blue">function</span> offsetCameraPos(source, pos, trg, leftToRight) {</p>    <p style="margin: 0px">&#160; <span style="color: green">// Use a small fraction of the distance for the camera offset</span></p>    <p style="margin: 0px">&#160; <span style="color: blue">var</span> disp = pos.distanceTo(trg) * 0.04;</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; <span style="color: green">// Clone the camera and return its X translated position</span></p>    <p style="margin: 0px">&#160; <span style="color: blue">var</span> clone = source.autocamCamera.clone();</p>    <p style="margin: 0px">&#160; clone.translateX(leftToRight ? disp : -disp);</p>    <p style="margin: 0px">&#160; <span style="color: blue">return</span> clone.position;</p>    <p style="margin: 0px">}</p>    <p style="margin: 0px">&#160;</p> </div>  <p>The very earliest implementation kept the views in sync by watching for “camera-changed” events on each and then transferring the view to the other view using the above function. For subsequent versions – while the original mode was left in – it made sense to have both views controlled by the tilting of the device.</p>  <p>HTML5 provides a very handy “deviceorientation” event that gives your application alpha, beta &amp; gamma (which can be mapped to roll, pitch &amp; yaw) values for the position of the device. This allows applications to determine the movement of the head – directionally, at least, even if it isn’t enough to provide full head-tracking.</p>  <p>There were a couple of navigation options available: we could have placed the model somewhere in 3D space and then when you happened to look in that direction you’d see it. This is probably better for architectural/site/cityscape navigation… it’s more of a walkthrough model of navigation.</p>  <p>As we’re fairly constrained with our model size, I opted rather for a widget-viewer navigation style: the object is on a virtual turntable and will rotate as you turn your head to the left/right (or up/down). I chose to constrain the rotation to be a turntable effect – effectively limiting the rotation to two axes – because it gave a better user experience than having full 3-axis rotations possible.</p>  <p>Here’s the deviceorientation event handler. Don’t worry about the specifics, just that it distills the device orientation data down to two values that get used to oribit the views themselves:</p>  <div style="font-size: 8pt; font-family: courier new; background: white; color: black; line-height: 140%">   <p style="margin: 0px"><span style="color: blue"></span></p>    <p style="margin: 0px"><span style="color: blue"></span></p>    <p style="margin: 0px"><span style="color: blue">function</span> orb(e) {</p>    <p style="margin: 0px">&#160; <span style="color: blue">if</span> (!e.alpha || !e.beta || !e.gamma || _updatingLeft || _updatingRight) <span style="color: blue">return</span>;</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; <span style="color: green">// Remove our handlers watching for camera updates,</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// as we'll make any changes manually</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// (we won't actually bother adding them back, afterwards,</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// as this means we're in mobile mode and probably inside</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// a Google Cardboard holder)</span></p>    <p style="margin: 0px">&#160; unwatchCameras();</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; <span style="color: green">// gamma is the front-to-back in degrees (with</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// this screen orientation) with +90/-90 being</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// vertical and negative numbers being 'downwards'</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// with positive being 'upwards'</span></p>    <p style="margin: 0px">&#160; <span style="color: blue">var</span> ab = Math.abs(e.beta);</p>    <p style="margin: 0px">&#160; <span style="color: blue">var</span> flipped = (ab &lt; 90 &amp;&amp; e.gamma &lt; 0) || (ab &gt; 90 &amp;&amp; e.gamma &gt; 0);</p>    <p style="margin: 0px">&#160; <span style="color: blue">var</span> vert = ((flipped ? e.gamma : -e.gamma) + (ab &lt; 90 ? 90 : -90)) * _deg2rad;</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; <span style="color: green">// When the orientation changes, reset the base direction</span></p>    <p style="margin: 0px">&#160; <span style="color: blue">if</span> (_wasFlipped != flipped) {</p>    <p style="margin: 0px">&#160;&#160;&#160; <span style="color: green">// If the angle goes below/above the horizontal, we don't</span></p>    <p style="margin: 0px">&#160;&#160;&#160; <span style="color: green">// flip direction (we let it go a bit further)</span></p>    <p style="margin: 0px">&#160;&#160;&#160; <span style="color: blue">if</span> (Math.abs(e.gamma) &lt; 45) {</p>    <p style="margin: 0px">&#160;&#160;&#160;&#160;&#160; flipped = _wasFlipped;</p>    <p style="margin: 0px">&#160;&#160;&#160; } <span style="color: blue">else</span> {</p>    <p style="margin: 0px">&#160;&#160;&#160;&#160;&#160; <span style="color: green">// Our base direction allows us to make relative horizontal</span></p>    <p style="margin: 0px">&#160;&#160;&#160;&#160;&#160; <span style="color: green">// rotations when we rotate left &amp; right</span></p>    <p style="margin: 0px">&#160;&#160;&#160;&#160;&#160; _wasFlipped = flipped;</p>    <p style="margin: 0px">&#160;&#160;&#160;&#160;&#160; _baseDir = e.alpha;</p>    <p style="margin: 0px">&#160;&#160;&#160; }</p>    <p style="margin: 0px">&#160; }</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; <span style="color: green">// alpha is the compass direction the device is</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// facing in degrees. This equates to the</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// left - right rotation in landscape</span></p>    <p style="margin: 0px">&#160; <span style="color: green">// orientation (with 0-360 degrees)</span></p>    <p style="margin: 0px">&#160; <span style="color: blue">var</span> horiz = (e.alpha - _baseDir) * _deg2rad;</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; <span style="color: green">// Save the latest horiz and vert values for use in zoom</span></p>    <p style="margin: 0px">&#160; _lastHoriz = horiz;</p>    <p style="margin: 0px">&#160; _lastVert = vert;</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160; orbitViews(vert, horiz);</p>    <p style="margin: 0px">}</p>    <p style="margin: 0px">&#160;</p>    <p style="margin: 0px">&#160;</p> </div>  <p>An interesting secondary – and very positive – effect of choosing this style of navigation is that, while feeling fairly natural, the brain doesn’t try to resolve the view changes with the real world. Which means you really don’t feel “VR sickness”… something typically caused by a phenomenon known as the <a href="http://informationdisplay.org/IDArchive/2012/FebruaryMarch/FrontlineTechnologyTowardtheUltimate3DDisp.aspx">vergence-accommodation conflict</a>, where the eyes accommodate to the screen but rotate to fix the apparent image.</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee233f970b-pi" target="_blank"><img title="Vergence Accommodation Conflict" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Vergence Accommodation Conflict" src="/assets/image_976775.jpg" width="218" height="142" /></a></p>  <p>Once the basic prototype was working, I went and added support for multiple models. Placing these in a separate menu had an additional advantage: in HTML5 it’s not possible to force a page to be viewed full-screen without there being some kind of user-driven event (unless the full-screen API is called from a UI event handler, it doesn’t do anything).</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201bb089243c6970d-pi" target="_blank"><img title="Front menu for the VR prototype" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Front menu for the VR prototype" src="/assets/image_509579.jpg" width="251" height="331" /></a></p>  <p>The fact that this prototype uses two separate viewer instances is clearly less than ideal – it would be much better to have a single viewer with a stereo renderer – but the question only really concerns scalability: our prototype works well enough for smaller models, and with a stereo renderer we could presumably work with models that are approaching twice the complexity.</p>  <p>You can try the basic implementation <a href="http://autode.sk/gcb">here</a>.</p>  <p><b></b></p>  <p><b>User input for VR</b></p>  <p>One of the major problems yet to be solved effectively for VR is how to manage user input. It’s clearly not possible to touch the screen of your smartphone while it’s inserted in a Google Cardboard holder, for instance.</p>  <p>People are attempting to address this is a number of ways, whether by measuring the time your gaze spends on a particular object, or attaching external peripherals such as Leap Motion or Kinect to recognize hand gestures or body movements while you’re immersed in a virtual space.</p>  <p>One possibility that I believe to have largely been overlooked is voice: there are straightforward – even web-based – voice recognition APIs that allow you to add voice commands easily to your VR environment. The second major iteration of the VR prototype made use of the Google Speech Recognition API – via a handy JavaScript wrapper called Annyang – to add support for a number of different voice commands: </p>  <p>EXPLODE, COMBINE, zoom IN and OUT, FRONT, BACK, LEFT, RIGHT, TOP, BOTTOM</p>  <p>The code making use of Annyang to implement a series of voice commands can be found <a href="http://safe-reef-1847.herokuapp.com/js/stereo-voice.js">here</a>. And <a href="https://youtu.be/0csk5bK8Iyk">here’s a YouTube video</a> showing the implementation in action.</p>  <p>&#160;</p>  <p><b>Using the SDKs for Google Cardboard and Gear VR with our web-based viewer</b></p>  <p>There are various benefits to creating a native Android application for Google Cardboard. For instance, the SDK gives you access to an event telling you when the magnet-based trigger has been pulled (although not all holders come with a magnetic trigger, these days), as well as automatically disabling the display “auto off” feature – something you otherwise will want to do manually when working with pure WebVR.</p>  <p>Other than that, it’s very feasible to have the Cardboard app load your web-page into an Android WebView – assuming the target device is running Android Lollipop or higher, as versions prior to this didn’t have WebGL support from their WebViews.</p>  <p>Here you can see some code inside Android Studio that shows how to respond to the trigger event:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b8d177e946970c-pi" target="_blank"><img title="Cardboard SDK project inside Android Studio" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Cardboard SDK project inside Android Studio" src="/assets/image_438942.jpg" width="449" height="333" /></a></p>  <p>The back() and enter() functions simply call JavaScript functions in the WebView using code such as this:</p>  <p><b><font size="3" face="Courier New">mWebView.loadUrl(&quot;javascript:openSelected()&quot;);</font></b></p>  <p>This is the general approach for native applications – whether targeting Cardboard or Gear VR – to call into the code behind their embedded web-pages.</p>  <p>Speaking of Gear VR, this is the Cardboard-like device being brought to market by Samsung. It benefits from having a runtime engine developed by Oculus – in fact it’s the way Oculus intends to address the needs of the broader consumer VR market – as well as having some excellent hardware: the first class optics with their 96 degree field of view are really a step up from the standard Cardboard offering.</p>  <p>There are also additional hardware buttons, as well as a separate Bluetooth gamepad:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b8d177e94a970c-pi" target="_blank"><img title="Gear VR hardware" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Gear VR hardware" src="/assets/image_672671.jpg" width="381" height="324" /></a></p>  <p>When using the Oculus Mobile SDK, you can also hook up these UI components to functions in your embedded page’s implementation, much as we saw previously when using the Google Cardboard SDK.</p>  <p>Both SDKs present some interesting possibilities for applications, being more directly tied to a particular hardware platform. <a href="http://autode.sk/gvr">Here’s a prototype UI</a> that displays a carrousel of models, which can be selected and opened via the magnetic trigger on Google Cardboard or the touchpad on Gear VR.</p>  <p><b>Implementing collaborative features in web-based VR</b></p>  <p>As mentioned previously, one of the big issues with VR relates to user input. Beyond voice, another avenue I explored with some colleagues at another VR Hackathon, earlier in the year, was to enable someone to help guide you through the VR experience.</p>  <p>Imagine…</p>  <p>The architect of your dream home showing your whole family around the 3D model, changing the features – perhaps cosmetic, perhaps even structural – based on your real-time feedback. They control which room you happen to be looking at, but each family member can be looking in the direction that interests them.</p>  <p>Or a doctor showing you a blown up view of an MRI of your knee, highlighting the specifics of your surgical options.</p>  <p>Or a teacher taking their class on a virtual field trip, visiting the Great Wall of China and the bottom of the Pacific Ocean, all in one day. (This last one was announced the week after the Hackathon… expect Google Expeditions to come to a classroom near you, soon!)</p>  <p>We took this concept – and the initial prototype I’d created 6 months before – and fleshed it out during the course of the Hackathon weekend. The result was a master page hosting the full View and Data GUI viewer (with the full A360 UI), than controls an arbitrary number of clients connected to its session. We had initially targeted a single client, but realised that if we did it right we’d get as many clients as we wanted, basically for free.</p>  <p>Here’s the main page:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b8d177e956970c-pi" target="_blank"><img title="Master view of our model" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Master view of our model" src="/assets/image_130152.jpg" width="427" height="333" /></a></p>  <p>Clients with a compatible device – pretty much anything running Chrome, although WebGL on iOS continues to be a bit hit or miss – can connect into the session by scanning the dynamically generated QR Code, at which point they’ll see the same model as the master:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201bb089243ce970d-pi" target="_blank"><img title="Prototype VR viewer" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="Prototype VR viewer" src="/assets/image_437289.jpg" width="504" height="277" /></a></p>  <p>An important difference being, you have control over the viewing angle of the model – you can inspect it from whatever direction you please.</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201bb089243d2970d-pi" target="_blank"><img title="A top view of our model" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="A top view of our model" src="/assets/image_646181.jpg" width="504" height="275" /></a></p>  <p>On the master page, any changes made by the controller are captured via events on the viewer component:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee2359970b-pi" target="_blank"><img title="About to isolate a component" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="About to isolate a component" src="/assets/image_517946.jpg" width="427" height="333" /></a></p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201bb089243d6970d-pi" target="_blank"><img title="And the component is isolated" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="And the component is isolated" src="/assets/image_166705.jpg" width="427" height="333" /></a></p>  <p>These then get propagated down to the various viewing clients:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee2361970b-pi" target="_blank"><img title="A top view of the base" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="A top view of the base" src="/assets/image_322626.jpg" width="504" height="275" /></a></p>  <p>Once the basic communications infrastructure was in place – we used Node.js on the server in conjunction with Socket.io for WebSocket communication between the master and the clients – it was trivial to extend support for the various operations. If you explode on the master or isolate components via the model tree, these changes will be seen on the various clients. The same is even true for sectioning.</p>  <p>The most difficult to implement ended up being zoom: we wanted the master to be able to control the zoom factor on the various clients, but not the view direction. So we had to jump through a few hoops to make that work properly.</p>  <p>One nice feature is the ability to upload your own models (although we set a cap at 2MB, to set expectations appropriately). For instance, you can head on over to the Herman Miller web-site and download models of their chairs (I found the best results, material-wise, with their 3ds Max models):</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee2369970b-pi" target="_blank"><img title="3D models on the Herman Miller site" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="3D models on the Herman Miller site" src="/assets/image_348465.jpg" width="500" height="328" /></a></p>  <p>Here’s the Sayl chair loaded into Vrok-It:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201b7c7ee236e970b-pi" target="_blank"><img title="The uploaded chair" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="The uploaded chair" src="/assets/image_731219.jpg" width="427" height="333" /></a></p>  <p>We can see the model loaded onto our various clients, too:</p>  <p><a href="http://through-the-interface.typepad.com/.a/6a00d83452464869e201bb089243ec970d-pi" target="_blank"><img title="The view on a client" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin: 50px auto; display: block; padding-right: 0px; border-top-width: 0px" border="0" alt="The view on a client" src="/assets/image_567437.jpg" width="504" height="305" /></a></p>  <p><a href="http://vrok.it" target="_blank">Vrok-It</a> is live, if you want to try it for yourself. You can also find the full source-code for the application <a href="https://github.com/KeanW/vr-party" target="_blank">posted to GitHub</a>.</p>  <p><em>A big thank you to Lars Schneider and Oleg Dedkow for their part in developing the Vrok-It application.</em></p>
