---
layout: "post"
title: "Capturing the human experience of the built environment"
date: "2023-11-22 09:24:33"
author: "Kean Walmsley"
categories:
  - "AU"
  - "Autodesk"
  - "Autodesk Research"
  - "Conferences"
  - "Human-centric building design"
original_url: "https://www.keanw.com/2023/11/capturing-the-human-experience-of-the-built-environment.html "
typepad_basename: "capturing-the-human-experience-of-the-built-environment"
typepad_status: "Publish"
---

<p>Now that the dust has settled on another Autodesk University (the best one ever?) it’s a good time to revisit what brought our team to AU, this year. I realize that I've posted a lot of pictures of people wearing helmets and high visibility vests without really explaining what it was all about.</p>
<p><a href="https://www.keanw.com/2022/09/human-centric-building-design.html" target="_blank" rel="noopener">Our team at Autodesk Research</a> has been very focused on understanding the human experience of the built environment, with the ultimate goal of making our software tools better at helping our customers design human-centric spaces. Spaces that emphasize occupant well-being in its various forms.</p>
<p>Humans are tricky: we’re all different. It’s possible to look at broad themes that make humans thrive, but there are always limits. For instance we all love biophilic design, at least until a plant starts blooming and someone who’s allergic to its pollen has a sneezing fit. We’re exploring how these broad themes can be applied during design, but that’s a separate area of our research: AU was about something we call “experiential walkthroughs”, which we’re using to capture our own subjective experiences.</p>
<p>Experiential walkthroughs use technology to help capture the human experience in an architectural or urban space. We fit people out with helmets that contain an array of sensors: a 360 camera mounted on top, a selfie camera focused on the participant’s face, a set of environmental sensors for ambient temperature etc. We also give the participant a phone that will prompt them with questions intermittently, and even ask them to solve a task or two. Capturing this qualitative data is a critical component of this process: sensors will only tell part of the story.</p>
<p><a href="/assets/image_668335.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Bentway helmet" src="/assets/image_668335.jpg" alt="Bentway helmet" width="375" height="500" border="0" /></a></p>
<p>We performed a series of “walkshops” in the summer in Toronto, asking study participants to capture their experience of <a href="https://thebentway.ca/">The Bentway</a>, an urban space beneath the <a href="https://en.wikipedia.org/wiki/Gardiner_Expressway">Gardiner Expressway</a> in Toronto that has been redeveloped as community space.</p>
<p><a href="/assets/image_290845.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Lily at The Bentway" src="/assets/image_290845.jpg" alt="Lily at The Bentway" width="500" height="375" border="0" /></a></p>
<p>We gathered around 550 GB of data during these walkshops, and synthesized it into an exhibit called <a href="https://thebentway.ca/event/from-steps-to-stories/">From Steps to Stories</a> - which I talked about in <a href="https://www.keanw.com/2023/10/more-information-on-from-steps-to-stories.html">this recent post</a> - that we presented in Toronto with our partners at The Bentway the week before AU.</p>
<p><a href="/assets/image_2226.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Our setup in Toronto" src="/assets/image_2226.jpg" alt="Our setup in Toronto" width="500" height="375" border="0" /></a></p>
<p><a href="/assets/image_473992.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Another view of Toronto" src="/assets/image_473992.jpg" alt="Another view of Toronto" width="500" height="375" border="0" /></a></p>
<p>This original exhibit used 6 vertically mounted 55” monitors to display the data, with a 27” touch screen serving as the input device.</p>
<p><a href="/assets/image_591184.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Bon using the Toronto exhibit" src="/assets/image_591184.jpg" alt="Bon using the Toronto exhibit" width="500" height="375" border="0" /></a></p>
<p>I wish I could have travelled to the opening of the exhibit in Toronto, but the timing would have meant multiple trans-atlantic trips in the space of a few weeks or an extremely long trip to AU etc. By all accounts it went really well, though.</p>
<p><a href="/assets/image_571213.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="The opening of From Steps to Stories" src="/assets/image_571213.jpg" alt="The opening of From Steps to Stories" width="500" height="375" border="0" /></a></p>
<p>When we brought the exhibit to AU 2023, we used 3 horizontally mounted 55” monitors with a Surface Pro 9 tablet (which was actually driving the 27” display in Toronto) as the input device.</p>
<p><a href="/assets/image_434877.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Our setup at AU 2023" src="/assets/image_434877.jpg" alt="Our setup at AU 2023" width="500" height="375" border="0" /></a></p>
<p>From a technical perspective, both displays (the main display and the controller) are actually web-pages: the main display in Toronto needed Chrome to have a special flag enabled to scale to a really high resolution, apparently. The “server” (the PC running the exhibit which has the various 55” monitors merged into an extended screen) also has a process checking for web socket input from the controller page. So as the touch-screen is used to drive the UI, the main page changes what it’s showing. Simple and neat.</p>
<p><a href="/assets/image_989581.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="Jacky showing off the exhibit" src="/assets/image_989581.jpg" alt="Jacky showing off the exhibit" width="500" height="375" border="0" /></a></p>
<p>Here’s a video of the exhibit in action. The controller is at the bottom, the main display at the top.</p>
<p style="text-align: center;"> </p>
<p style="text-align: center;"><iframe title="YouTube video player" src="https://www.youtube.com/embed/fnpD_4a85No?si=tCwelRbXB1dl1skK" width="500" height="283" frameborder="0" allowfullscreen=""></iframe></p>
<p style="text-align: center;"> </p>
<p style="text-align: left;">While people could explore the data captured at The Bentway during AU 2023, we also wanted to capture data about the Autodesk University exhibit hall experience and provide insights from that data to the AU team.</p>
<p style="text-align: left;"><a href="/assets/image_750669.jpg" target="_blank" rel="noopener"><img style="display: block; margin: 30px auto;" title="A study participant donning a helmet at AU 2023" src="/assets/image_750669.jpg" alt="A study participant donning a helmet at AU 2023" width="500" height="375" border="0" /></a></p>
<p style="text-align: left;">During the course of the 3 days, we had approximately 40 people don data capture helmets and walk across the expo to the “Sustainability Forest” and back, sharing their feedback on the experience via a smart phone (as well as us capturing their individual choices of route, etc.).</p>
<p style="text-align: left;">While the visualization of this data could well be interesting - in much the same was as it has been for The Bentway - the larger opportunity is to start modeling the human experience of space via some form of Machine Learning. This type of dataset will eventually prove very helpful when designing future spaces.</p>
<p style="text-align: left;"><em>The Autodesk Research team involved in the creation of this project:</em></p>
<p style="text-align: left;"><em>Ray Wang, Frederik Brudy, Bon Aseniero, Sebastian Herrera, Mike Lee, Jacky Bibliowicz, Ellen Hlozan, Matthew Spremulli, Pan Zhang, Liviu Calin, Lorenzo Villaggi, Brian Lee, Jamie Nicholson, Dagmara Szkurlat and myself.</em><br /><br /><em>Many thanks to others from Autodesk Research who also contributed to the running of walkshops and our exhibit at AU:</em></p>
<p style="text-align: left;"><em>Elliott Montgomery, Lily Prasuethsut, Athena Moore, Alanna Mongkhounsavath, Yi Wang, Allin Groom and Qian Zhou.</em></p>
